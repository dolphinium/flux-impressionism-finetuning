{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux Impressionism Fine-Tuning Training\n",
    "\n",
    "This notebook implements the LoRA fine-tuning process for the Flux.1 Dev model with int4 + bf16 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(\"..\")\n",
    "from src.training.trainer import FluxLoRATrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "from transformers import PreTrainedModel, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import FluxModel, FluxScheduler\n",
    "from datasets import Dataset\n",
    "\n",
    "class FluxLoRATrainer:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.setup_logging()\n",
    "        self.setup_accelerator()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging for the training process.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            level=logging.INFO,\n",
    "            handlers=[\n",
    "                logging.StreamHandler(),\n",
    "                logging.FileHandler(os.path.join(self.config[\"output\"][\"logging_dir\"], \"training.log\"))\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_accelerator(self):\n",
    "        \"\"\"Configure accelerator for distributed training.\"\"\"\n",
    "        project_config = ProjectConfiguration(\n",
    "            project_dir=self.config[\"output\"][\"output_dir\"],\n",
    "            logging_dir=self.config[\"output\"][\"logging_dir\"]\n",
    "        )\n",
    "        \n",
    "        self.accelerator = Accelerator(\n",
    "            gradient_accumulation_steps=self.config[\"training\"][\"gradient_accumulation_steps\"],\n",
    "            mixed_precision=\"bf16\" if self.config[\"mixed_precision\"][\"enabled\"] else \"no\",\n",
    "            project_config=project_config,\n",
    "            log_with=self.config[\"output\"][\"report_to\"]\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load and prepare the Flux model with LoRA configuration.\"\"\"\n",
    "        # Load base model with quantization\n",
    "        model = FluxModel.from_pretrained(\n",
    "            self.config[\"model\"][\"pretrained_model_name_or_path\"],\n",
    "            torch_dtype=getattr(torch, self.config[\"model\"][\"torch_dtype\"]),\n",
    "            load_in_4bit=self.config[\"model\"][\"load_in_4bit\"],\n",
    "            use_bf16_4bit=self.config[\"model\"][\"use_bf16_4bit\"],\n",
    "            bnb_4bit_compute_dtype=getattr(torch, self.config[\"model\"][\"bnb_4bit_compute_dtype\"]),\n",
    "            bnb_4bit_quant_type=self.config[\"model\"][\"bnb_4bit_quant_type\"],\n",
    "            bnb_4bit_use_double_quant=self.config[\"model\"][\"bnb_4bit_use_double_quant\"]\n",
    "        )\n",
    "\n",
    "        # Prepare model for k-bit training\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model,\n",
    "            use_gradient_checkpointing=self.config[\"system\"][\"gradient_checkpointing\"]\n",
    "        )\n",
    "\n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config[\"lora\"][\"rank\"],\n",
    "            lora_alpha=self.config[\"lora\"][\"alpha\"],\n",
    "            target_modules=self.config[\"lora\"][\"target_modules\"],\n",
    "            lora_dropout=self.config[\"lora\"][\"lora_dropout\"],\n",
    "            bias=self.config[\"lora\"][\"bias\"]\n",
    "        )\n",
    "\n",
    "        # Apply LoRA\n",
    "        self.model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        if self.config[\"system\"][\"enable_xformers_memory_efficient_attention\"]:\n",
    "            self.model.enable_xformers_memory_efficient_attention()\n",
    "        \n",
    "        if self.config[\"system\"][\"use_flash_attention_2\"]:\n",
    "            self.model.enable_flash_attention_2()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def prepare_dataset(self, dataset: Dataset):\n",
    "        \"\"\"Prepare dataset for training.\"\"\"\n",
    "        if self.config[\"dataset\"][\"max_train_samples\"]:\n",
    "            dataset = dataset.select(range(self.config[\"dataset\"][\"max_train_samples\"]))\n",
    "        \n",
    "        # Add any necessary preprocessing here\n",
    "        return dataset\n",
    "\n",
    "    def train(self, dataset: Dataset):\n",
    "        \"\"\"Execute the training loop.\"\"\"\n",
    "        # Prepare training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config[\"output\"][\"output_dir\"],\n",
    "            per_device_train_batch_size=self.config[\"training\"][\"train_batch_size\"],\n",
    "            gradient_accumulation_steps=self.config[\"training\"][\"gradient_accumulation_steps\"],\n",
    "            learning_rate=self.config[\"training\"][\"learning_rate\"],\n",
    "            lr_scheduler_type=self.config[\"training\"][\"lr_scheduler\"],\n",
    "            num_train_epochs=self.config[\"training\"][\"num_train_epochs\"],\n",
    "            max_steps=self.config[\"training\"][\"max_train_steps\"],\n",
    "            warmup_steps=self.config[\"training\"][\"lr_warmup_steps\"],\n",
    "            save_steps=self.config[\"training\"][\"checkpointing_steps\"],\n",
    "            save_total_limit=self.config[\"training\"][\"save_total_limit\"],\n",
    "            logging_steps=10,\n",
    "            remove_unused_columns=False,\n",
    "            seed=self.config[\"training\"][\"seed\"],\n",
    "            bf16=self.config[\"mixed_precision\"][\"enabled\"],\n",
    "            report_to=self.config[\"output\"][\"report_to\"]\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            data_collator=self.collate_fn\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        self.logger.info(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self, path: Optional[str] = None):\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        save_path = path or os.path.join(self.config[\"output\"][\"output_dir\"], \"final_model\")\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(examples):\n",
    "        \"\"\"Collate function for batch preparation.\"\"\"\n",
    "        # Implement custom collate function here\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(\"../configs/default.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config[\"output\"][\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"output\"][\"logging_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = FluxLoRATrainer(config)\n",
    "\n",
    "# Load and prepare model\n",
    "model = trainer.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(config[\"dataset\"][\"name\"])\n",
    "train_dataset = trainer.prepare_dataset(dataset[\"train\"])\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete\n",
    "\n",
    "The fine-tuned model has been saved to the output directory. You can now use it for inference or upload it to the Hugging Face Hub."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
