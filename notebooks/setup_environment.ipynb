{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup for Flux Impressionism Fine-Tuning\n",
    "\n",
    "This notebook sets up the training environment for fine-tuning Flux.1 Dev model with LoRA for Impressionist style transfer.\n",
    "\n",
    "## Steps:\n",
    "1. Check GPU availability\n",
    "2. Install dependencies\n",
    "3. Configure HuggingFace access\n",
    "4. Set up logging and monitoring\n",
    "5. Verify dataset access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q torch==2.1.0 torchvision==0.16.0\n",
    "!pip install -q transformers==4.36.0 diffusers==0.25.0 accelerate==0.25.0\n",
    "!pip install -q bitsandbytes==0.41.0 tensorboard==2.15.0\n",
    "!pip install -q datasets==2.15.0 huggingface-hub==0.19.0\n",
    "!pip install -q pillow==10.0.0 numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import and verify package installation\n",
    "import torch\n",
    "import transformers\n",
    "import diffusers\n",
    "import accelerate\n",
    "import bitsandbytes as bnb\n",
    "import datasets\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Diffusers version: {diffusers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Total GPU memory: {gpu_memory:.2f} GB\")\n",
    "    \n",
    "    # Determine appropriate quantization based on available memory\n",
    "    if gpu_memory >= 18:\n",
    "        print(\"Recommended quantization: int8 + bf16 (full precision)\")\n",
    "    elif gpu_memory >= 13:\n",
    "        print(\"Recommended quantization: int4 + bf16\")\n",
    "    elif gpu_memory >= 9:\n",
    "        print(\"Recommended quantization: NF4/int2 + bf16\")\n",
    "    else:\n",
    "        print(\"Warning: Available GPU memory might be insufficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Authentication\n",
    "\n",
    "1. Go to https://huggingface.co/settings/tokens\n",
    "2. Create a new token with write access\n",
    "3. Copy the token and use it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# HuggingFace authentication\n",
    "from getpass import getpass\n",
    "\n",
    "hf_token = getpass(\"Enter your HuggingFace token: \")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify dataset access\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"dolphinium/wikiart-impressionism-curated\")\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Number of images: {len(dataset['train'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up TensorBoard\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Clear any existing logs\n",
    "!rm -rf {log_dir}/*\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Verification Complete\n",
    "\n",
    "If all cells above ran successfully, your environment is properly configured for training. The setup includes:\n",
    "\n",
    "✅ GPU availability and memory check\n",
    "✅ Required packages installation\n",
    "✅ HuggingFace authentication\n",
    "✅ Dataset access verification\n",
    "✅ TensorBoard setup for monitoring\n",
    "\n",
    "Next steps:\n",
    "1. Implement training pipeline\n",
    "2. Set up evaluation metrics\n",
    "3. Create sample generation system"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}